{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9836d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neural_network import MLP\n",
    "from typing import Sequence, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06c5e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28)\n",
      "Y_train: (60000,)\n",
      "X_test:  (10000, 28, 28)\n",
      "Y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "print(f'X_train: {train_X.shape}')\n",
    "print(f'Y_train: {train_y.shape}')\n",
    "print(f'X_test:  {test_X.shape}')\n",
    "print(f'Y_test:  {test_y.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e21e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x: np.ndarray) -> np.ndarray: # (B, C) -> (B -> C)\n",
    "    \"\"\"\n",
    "    Applies the logistic function to an batched sums\n",
    "    :param x: an input array\n",
    "    :return: An array of the same size\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-1 * x))\n",
    "\n",
    "\n",
    "def logistic_prime(x: np.ndarray) -> np.ndarray: # (B, C) -> (B ->C)\n",
    "    \"\"\"\n",
    "    Applies the derivative of the logistic function to batched sums\n",
    "    :param x: an input array\n",
    "    :return: An array of the same size\n",
    "    \"\"\"\n",
    "    normal_logistic = 1.0 / (1.0 + np.exp(-1 * x))  # efficiency\n",
    "    return normal_logistic * (1 - normal_logistic)\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes: Sequence[int]):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.biases = [np.random.randn(y) for y in layer_sizes[1:]]\n",
    "        # x is the size of the output from the previous layer, y is the size of the layer the weight outputs are going to\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "    def feedforward(self, x: np.ndarray) -> np.ndarray: # (B, input_layer_size) -> (B, output_layer_size)\n",
    "        \"\"\"\n",
    "        Performs a feedforward run of the network and returns the output\n",
    "        :param x: An array of equal size to the first layer in the MLP, with shape (B, I)\n",
    "        :return: An array of equal size to the last layer in the MLP, with shape (B, O)\n",
    "        \"\"\"\n",
    "        activation = x # (B, I)\n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            activation = logistic((activation @ weight.T) + bias) # (B, previous layer size) -> (B, current layer size)\n",
    "        return activation\n",
    "\n",
    "    def sgd(self, train_x: List[np.ndarray], train_y: List[np.ndarray], test_x: List[np.ndarray],\n",
    "            test_y: List[np.ndarray], epochs: int, batch_size: int, learning_rate: float = 0.2) -> None:\n",
    "        \"\"\"\n",
    "        Performs stochastic gradient descent using training data, while\n",
    "        :param train_x: Training inputs\n",
    "        :param train_y: Training target\n",
    "        :param test_x: Testing inputs\n",
    "        :param test_y: Testing targets\n",
    "        :param epochs: The number of training runs to perform\n",
    "        :param batch_size: Size of each training batch before update\n",
    "        :param learning_rate: Learning rate to use when updating the model's parameters\n",
    "        \"\"\"\n",
    "        for i in range(epochs):\n",
    "            # training\n",
    "            batches = ((train_x[j: j + batch_size, :], train_y[j: j + batch_size, :])\n",
    "                       for j in range(0, len(train_x), batch_size))\n",
    "            for x, y in batches:\n",
    "                delta_weights, delta_biases = self.backprop(x, y)\n",
    "                # performing the updates\n",
    "                self.weights = [weight - (learning_rate / batch_size) * weight_update.sum(0)\n",
    "                                for weight, weight_update in zip(self.weights, delta_weights)]\n",
    "                self.biases = [bias - (learning_rate / batch_size) * bias_update.sum(0)\n",
    "                               for bias, bias_update in zip(self.biases, delta_biases)]\n",
    "            # running tests to track performance based on accuracy\n",
    "            prediction = self.feedforward(test_x) # (B, o), for predictions and y\n",
    "            correct = np.sum(np.argmax(prediction, 1) == np.argmax(test_y, 1))\n",
    "            print(f\"Accuracy for epoch {i}: {correct / len(test_x)}\")\n",
    "\n",
    "    def backprop(self, x: np.ndarray, y: np.ndarray) -> (List[np.ndarray], List[np.ndarray]):\n",
    "        \"\"\"\n",
    "        Runs backprop for a single example through the network\n",
    "        :param x: An input with shape (B, I), where I is the size of the first layer\n",
    "        :param y: A target output of the input with shape (B, O), where O is the size of the last layer\n",
    "        :return: The weight and bias rates of change with respect to the cost averaged over the full batch\n",
    "        \"\"\"\n",
    "        activations = [x] # list[(B, I)] to start each entry will be (B, l), where l is the size of the current layer\n",
    "        zs = []\n",
    "        delta_weights = [] # list of (B, l, l-1)\n",
    "        delta_biases = [] # list of (B, l)\n",
    "        # feedforward to collect activations as the neuron sums (zs)\n",
    "        for weight, bias in zip(self.weights, self.biases): # weight (l, l-1), bias (l) where l is the size of the current layer\n",
    "            # for the first layer I = l-1, so for each layer we do (B, l-1) @ (l, l-1).T = (B, l)\n",
    "            zs.append(activations[-1] @ weight.T + bias)\n",
    "            activations.append(logistic(zs[-1]))\n",
    "        # backprop step\n",
    "        delta_a = activations[-1] - y  # taking the derivative of the cost for each activation\n",
    "        delta_z = delta_a * logistic_prime(activations[-1])  # back propagating from the last output to the last sum\n",
    "        delta_biases.append(delta_z)  # the differentials for biases are equal to the delta for the sum\n",
    "        # for weights we need (B, L, 1) (B, 1, L-1) @ (B, L, L-1)\n",
    "        delta_weights.append(delta_z[:, :, np.newaxis] @ activations[-2][:, np.newaxis, :])  # calculating differentials for weights\n",
    "        # continuing to backpropagation through each layer\n",
    "        for i in range(2, len(self.weights) + 1):\n",
    "            delta_a = delta_z @ self.weights[-i + 1] # (B, l + 1) @ (l+1, l).T = (B, l)\n",
    "            delta_z = delta_a * logistic_prime(activations[-i])  # backpropagation from the activation to the sum\n",
    "            delta_biases.append(delta_z)\n",
    "            delta_weights.append(delta_z[:, :, np.newaxis] @ activations[-i - 1][:, np.newaxis, :]) # (B, l, 1) @ (B, 1, l-1) = (B, l, l-1)\n",
    "        delta_weights.reverse()\n",
    "        delta_biases.reverse()\n",
    "        return delta_weights, delta_biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c570563",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = MLP([28*28, 30, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5be3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flattened = train_X.reshape(-1, 28*28)\n",
    "test_x_flattened = test_X.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd6b0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_vectorized = np.eye(10)[train_y]\n",
    "test_y_vectorized = np.eye(10)[test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc87f3b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17033\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for epoch 0: 0.347\n",
      "Accuracy for epoch 1: 0.424\n",
      "Accuracy for epoch 2: 0.4757\n",
      "Accuracy for epoch 3: 0.4714\n",
      "Accuracy for epoch 4: 0.518\n",
      "Accuracy for epoch 5: 0.5476\n",
      "Accuracy for epoch 6: 0.553\n",
      "Accuracy for epoch 7: 0.5791\n",
      "Accuracy for epoch 8: 0.5952\n",
      "Accuracy for epoch 9: 0.6019\n",
      "Accuracy for epoch 10: 0.6045\n",
      "Accuracy for epoch 11: 0.6142\n",
      "Accuracy for epoch 12: 0.6201\n",
      "Accuracy for epoch 13: 0.6377\n",
      "Accuracy for epoch 14: 0.6407\n",
      "Accuracy for epoch 15: 0.6545\n",
      "Accuracy for epoch 16: 0.6564\n",
      "Accuracy for epoch 17: 0.6625\n",
      "Accuracy for epoch 18: 0.6581\n",
      "Accuracy for epoch 19: 0.6617\n"
     ]
    }
   ],
   "source": [
    "my_nn.sgd(train_x_flattened[:10000], train_y_vectorized[:10000], test_x_flattened, test_y_vectorized, epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b343d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
